{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4UC4ufKanma"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Define paths to sample images\n",
        "upic = 'D:/cell_images/Uninfected/C100P61ThinF_IMG_20150918_144104_cell_131.png'\n",
        "apic = 'D:/cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_164.png'\n",
        "\n",
        "# Display sample images\n",
        "plt.figure(1, figsize=(15, 7))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.imread(upic))\n",
        "plt.title('Uninfected Cell')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.imread(apic))\n",
        "plt.title('Infected Cell')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Define image dimensions\n",
        "width = 128\n",
        "height = 128\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation and splitting\n",
        "datagen = ImageDataGenerator(rescale=1 / 255.0, validation_split=0.2)  # Rescale pixel values and split data\n",
        "\n",
        "# Create train and validation data generators\n",
        "trainDatagen = datagen.flow_from_directory(\n",
        "    directory='D:/cell_images/cell_images/',  # Path to the dataset directory\n",
        "    target_size=(width, height),  # Resize images to the specified dimensions\n",
        "    class_mode='binary',  # Set class mode to binary for classification\n",
        "    batch_size=16,  # Set batch size for training\n",
        "    subset='training'  # Use the training subset of the data\n",
        ")\n",
        "\n",
        "valDatagen = datagen.flow_from_directory(\n",
        "    directory='D:/cell_images/cell_images/',  # Path to the dataset directory\n",
        "    target_size=(width, height),  # Resize images to the specified dimensions\n",
        "    class_mode='binary',  # Set class mode to binary for classification\n",
        "    batch_size=16,  # Set batch size for validation\n",
        "    subset='validation'  # Use the validation subset of the data\n",
        ")\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 3)))  # Convolutional layer with 16 filters\n",
        "model.add(MaxPool2D(2, 2))  # Max pooling layer\n",
        "model.add(Dropout(0.2))  # Dropout layer for regularization\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))  # Convolutional layer with 32 filters\n",
        "model.add(MaxPool2D(2, 2))  # Max pooling layer\n",
        "model.add(Dropout(0.3))  # Dropout layer for regularization\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))  # Convolutional layer with 64 filters\n",
        "model.add(MaxPool2D(2, 2))  # Max pooling layer\n",
        "model.add(Dropout(0.3))  # Dropout layer for regularization\n",
        "\n",
        "model.add(Flatten())  # Flatten the output for the dense layers\n",
        "model.add(Dense(64, activation='relu'))  # Dense layer with 64 units\n",
        "model.add(Dropout(0.5))  # Dropout layer for regularization\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)  # Stop training if validation loss doesn't improve for 2 epochs\n",
        "\n",
        "# Train the model\n",
        "history = model.fit_generator(\n",
        "    generator=trainDatagen,  # Training data generator\n",
        "    steps_per_epoch=len(trainDatagen),  # Number of steps per epoch\n",
        "    epochs=20,  # Number of training epochs\n",
        "    validation_data=valDatagen,  # Validation data generator\n",
        "    validation_steps=len(valDatagen),  # Number of validation steps\n",
        "    callbacks=[early_stop]  # Early stopping callback\n",
        ")\n",
        "\n",
        "# Define a function to plot the learning curves\n",
        "def plotLearningCurve(history, epochs):\n",
        "    epochRange = range(1, epochs + 1)\n",
        "    plt.plot(epochRange, history.history['accuracy'])\n",
        "    plt.plot(epochRange, history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochRange, history.history['loss'])\n",
        "    plt.plot(epochRange, history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "# Plot the learning curves for 3 epochs\n",
        "plotLearningCurve(history, 3)\n",
        "\n",
        "# Test a random image\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = image.load_img(\n",
        "    'C:/Users/Priya/Desktop/abc.png',  # Path to the test image\n",
        "    target_size=(128, 128)  # Resize the image\n",
        ")\n",
        "\n",
        "test_image = image.img_to_array(test_image)  # Convert image to array\n",
        "test_image = np.expand_dims(test_image, axis=0)  # Add an extra dimension\n",
        "\n",
        "result = model.predict(test_image)  # Make a prediction\n",
        "\n",
        "# Determine the prediction based on the result\n",
        "if result[0][0] >= 0.5:\n",
        "    prediction = \"Uninfected\"\n",
        "else:\n",
        "    prediction = \"Parasitized\"\n",
        "print(prediction)  # Print the prediction\n",
        "\n",
        "# Print class indices and lengths of train and validation data generators\n",
        "print(trainDatagen.class_indices)\n",
        "print(len(trainDatagen))\n",
        "print(1378 * 16)\n",
        "print(len(valDatagen))\n"
      ]
    }
  ]
}